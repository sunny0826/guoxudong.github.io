<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Guo Xudong's Blog·郭旭东的博客 – 图解 k8s</title><link>https://guoxudong.io/categories/%E5%9B%BE%E8%A7%A3-k8s/</link><description>Recent content in 图解 k8s on Guo Xudong's Blog·郭旭东的博客</description><generator>Hugo -- gohugo.io</generator><language>zh</language><lastBuildDate>Wed, 14 Oct 2020 10:28:06 +0800</lastBuildDate><atom:link href="https://guoxudong.io/categories/%E5%9B%BE%E8%A7%A3-k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>Post: 图解 K8S 源码 - QoS 篇</title><link>https://guoxudong.io/post/diagrams-k8s-src-qos/</link><pubDate>Wed, 14 Oct 2020 10:28:06 +0800</pubDate><guid>https://guoxudong.io/post/diagrams-k8s-src-qos/</guid><description>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>日常使用 Kubernetes 时，时长会出现 Node 节点中的 Pod 被 OOMKill 掉的情况，但 Node 节点中 Pod 众多，为什么单单选中这个 Pod Kill 掉呢？这里就引出了 QoS 的概念，本篇文章就会从源码的角度介绍 QoS 的分类、打分机制，并简单介绍不同 QoS 的本质区别。看看这个机制是如何保证运行在 Kubernetes 中服务质量的。&lt;/p>
&lt;h2 id="qos">QoS&lt;/h2>
&lt;p>QoS(Quality of Service) 即服务质量，是 Kubernetes 中的一种控制机制，其会对运行在 Kubernetes 中的 Pod 进行一个质量划分，根据 Pod 中 container 的 Limit 和 request 将 Pod 分为 &lt;code>Guaranteed&lt;/code>，&lt;code>Burstable&lt;/code>，&lt;code>BestEffort&lt;/code> 三类并对所有 Pod 进行一个打分。在资源尤其是内存这种不可压缩资源不够时，为保证整体质量的稳定，Kubernetes 就会根据 QoS 的不同优先级，对 Pod 进行资源回收。这也是有时集群中的 Pod 突然被 kill 掉的原因。&lt;/p>
&lt;p>&lt;img src="https://tva2.sinaimg.cn/large/ad5fbf65ly1gjotxqvm4uj20mr0aw78n.jpg" alt="Qos">&lt;/p>
&lt;h3 id="qos-分类">QoS 分类&lt;/h3>
&lt;p>以下代码用来获取 Pod 的 QoS 类，用于区分不同 Pod 的 QoS。&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#999;font-style:italic">// github/kubernetes/pkg/apis/core/v1/helper/qos/qos.go
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">func&lt;/span> &lt;span style="color:#447fcf">GetPodQOS&lt;/span>(pod *v1.Pod) v1.PodQOSClass {
requests := v1.ResourceList{}
limits := v1.ResourceList{}
zeroQuantity := resource.&lt;span style="color:#447fcf">MustParse&lt;/span>(&lt;span style="color:#ed9d13">&amp;#34;0&amp;#34;&lt;/span>)
isGuaranteed := &lt;span style="color:#6ab825;font-weight:bold">true&lt;/span>
allContainers := []v1.Container{}
allContainers = &lt;span style="color:#24909d">append&lt;/span>(allContainers, pod.Spec.Containers...)
&lt;span style="color:#999;font-style:italic">// InitContainers 容器也会被加入 QoS 的计算中
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> allContainers = &lt;span style="color:#24909d">append&lt;/span>(allContainers, pod.Spec.InitContainers...)
&lt;span style="color:#6ab825;font-weight:bold">for&lt;/span> _, container := &lt;span style="color:#6ab825;font-weight:bold">range&lt;/span> allContainers {
&lt;span style="color:#999;font-style:italic">// 遍历 requests 中的 CPU 和 memory 值
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">for&lt;/span> name, quantity := &lt;span style="color:#6ab825;font-weight:bold">range&lt;/span> container.Resources.Requests {
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> !&lt;span style="color:#447fcf">isSupportedQoSComputeResource&lt;/span>(name) {
&lt;span style="color:#6ab825;font-weight:bold">continue&lt;/span>
}
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> quantity.&lt;span style="color:#447fcf">Cmp&lt;/span>(zeroQuantity) == &lt;span style="color:#3677a9">1&lt;/span> {
delta := quantity.&lt;span style="color:#447fcf">DeepCopy&lt;/span>()
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> _, exists := requests[name]; !exists {
requests[name] = delta
} &lt;span style="color:#6ab825;font-weight:bold">else&lt;/span> {
delta.&lt;span style="color:#447fcf">Add&lt;/span>(requests[name])
requests[name] = delta
}
}
}
&lt;span style="color:#999;font-style:italic">// 遍历 limits 中的 CPU 和 memory 值
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> qosLimitsFound := sets.&lt;span style="color:#447fcf">NewString&lt;/span>()
&lt;span style="color:#6ab825;font-weight:bold">for&lt;/span> name, quantity := &lt;span style="color:#6ab825;font-weight:bold">range&lt;/span> container.Resources.Limits {
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> !&lt;span style="color:#447fcf">isSupportedQoSComputeResource&lt;/span>(name) {
&lt;span style="color:#6ab825;font-weight:bold">continue&lt;/span>
}
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> quantity.&lt;span style="color:#447fcf">Cmp&lt;/span>(zeroQuantity) == &lt;span style="color:#3677a9">1&lt;/span> {
qosLimitsFound.&lt;span style="color:#447fcf">Insert&lt;/span>(&lt;span style="color:#24909d">string&lt;/span>(name))
delta := quantity.&lt;span style="color:#447fcf">DeepCopy&lt;/span>()
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> _, exists := limits[name]; !exists {
limits[name] = delta
} &lt;span style="color:#6ab825;font-weight:bold">else&lt;/span> {
delta.&lt;span style="color:#447fcf">Add&lt;/span>(limits[name])
limits[name] = delta
}
}
}
&lt;span style="color:#999;font-style:italic">// 判断是否同时设置了 limits 和 requests，如果没有则不是 Guaranteed
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> !qosLimitsFound.&lt;span style="color:#447fcf">HasAll&lt;/span>(&lt;span style="color:#24909d">string&lt;/span>(v1.ResourceMemory), &lt;span style="color:#24909d">string&lt;/span>(v1.ResourceCPU)) {
isGuaranteed = &lt;span style="color:#6ab825;font-weight:bold">false&lt;/span>
}
}
&lt;span style="color:#999;font-style:italic">// 如果 requests 和 limits 都没有设置，则为 BestEffort
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> &lt;span style="color:#24909d">len&lt;/span>(requests) == &lt;span style="color:#3677a9">0&lt;/span> &amp;amp;&amp;amp; &lt;span style="color:#24909d">len&lt;/span>(limits) == &lt;span style="color:#3677a9">0&lt;/span> {
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> v1.PodQOSBestEffort
}
&lt;span style="color:#999;font-style:italic">// 检查所有资源的 requests 和 limits 是否都相等
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> isGuaranteed {
&lt;span style="color:#6ab825;font-weight:bold">for&lt;/span> name, req := &lt;span style="color:#6ab825;font-weight:bold">range&lt;/span> requests {
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> lim, exists := limits[name]; !exists || lim.&lt;span style="color:#447fcf">Cmp&lt;/span>(req) != &lt;span style="color:#3677a9">0&lt;/span> {
isGuaranteed = &lt;span style="color:#6ab825;font-weight:bold">false&lt;/span>
&lt;span style="color:#6ab825;font-weight:bold">break&lt;/span>
}
}
}
&lt;span style="color:#999;font-style:italic">// 都设置了 requests 和 limits，则为 Guaranteed
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> isGuaranteed &amp;amp;&amp;amp;
&lt;span style="color:#24909d">len&lt;/span>(requests) == &lt;span style="color:#24909d">len&lt;/span>(limits) {
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> v1.PodQOSGuaranteed
}
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> v1.PodQOSBurstable
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="qos-打分">QoS 打分&lt;/h3>
&lt;p>QoS 会根据不同的分类进行 OOMScore 打分，当宿主机上内存不足时，系统会优先 kill 掉 OOMScore 分数高的进程。&lt;/p>
&lt;p>&lt;img src="https://tvax3.sinaimg.cn/large/ad5fbf65ly1gjoqlfe8cgj20mr0pcqmy.jpg" alt="QoS 打分">&lt;/p>
&lt;p>值得注意的是不久之前 &lt;code>guaranteedOOMScoreAdj&lt;/code> 的值还是 &lt;code>-998&lt;/code>，今年 9 月 22 日才合并 &lt;a href="https://github.com/kubernetes/kubernetes/pull/71269">PR&lt;/a> 将其修改为 &lt;code>-997&lt;/code>，而修改的 PR 及 &lt;a href="https://github.com/kubernetes/kubernetes/issues/72294">相关 ISSUE&lt;/a> 在 2018 年就已经提出了，感兴趣的同学可以去看看。这里附上源码：&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#999;font-style:italic">// github/kubernetes/pkg/kubelet/qos/policy.go
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">const&lt;/span> (
&lt;span style="color:#999;font-style:italic">// KubeletOOMScoreAdj is the OOM score adjustment for Kubelet
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> KubeletOOMScoreAdj &lt;span style="color:#6ab825;font-weight:bold">int&lt;/span> = -&lt;span style="color:#3677a9">999&lt;/span>
&lt;span style="color:#999;font-style:italic">// KubeProxyOOMScoreAdj is the OOM score adjustment for kube-proxy
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> KubeProxyOOMScoreAdj &lt;span style="color:#6ab825;font-weight:bold">int&lt;/span> = -&lt;span style="color:#3677a9">999&lt;/span>
guaranteedOOMScoreAdj &lt;span style="color:#6ab825;font-weight:bold">int&lt;/span> = -&lt;span style="color:#3677a9">997&lt;/span>
besteffortOOMScoreAdj &lt;span style="color:#6ab825;font-weight:bold">int&lt;/span> = &lt;span style="color:#3677a9">1000&lt;/span>
)
&lt;span style="color:#6ab825;font-weight:bold">func&lt;/span> &lt;span style="color:#447fcf">GetContainerOOMScoreAdjust&lt;/span>(pod *v1.Pod, container *v1.Container, memoryCapacity &lt;span style="color:#6ab825;font-weight:bold">int64&lt;/span>) &lt;span style="color:#6ab825;font-weight:bold">int&lt;/span> {
&lt;span style="color:#999;font-style:italic">// 高优先级 Pod 直接返回 guaranteedOOMScoreAdj
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> types.&lt;span style="color:#447fcf">IsCriticalPod&lt;/span>(pod) {
&lt;span style="color:#999;font-style:italic">// Critical pods should be the last to get killed.
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> guaranteedOOMScoreAdj
}
&lt;span style="color:#999;font-style:italic">// 根据 QoS 等级，返回 guaranteedOOMScoreAdj 或 besteffortOOMScoreAdj 的分数，这里只处理 Guaranteed 与 BestEffort
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">switch&lt;/span> v1qos.&lt;span style="color:#447fcf">GetPodQOS&lt;/span>(pod) {
&lt;span style="color:#6ab825;font-weight:bold">case&lt;/span> v1.PodQOSGuaranteed:
&lt;span style="color:#999;font-style:italic">// Guaranteed containers should be the last to get killed.
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> guaranteedOOMScoreAdj
&lt;span style="color:#6ab825;font-weight:bold">case&lt;/span> v1.PodQOSBestEffort:
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> besteffortOOMScoreAdj
}
memoryRequest := container.Resources.Requests.&lt;span style="color:#447fcf">Memory&lt;/span>().&lt;span style="color:#447fcf">Value&lt;/span>()
&lt;span style="color:#999;font-style:italic">// 内存占用越少，分数越高
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> oomScoreAdjust := &lt;span style="color:#3677a9">1000&lt;/span> - (&lt;span style="color:#3677a9">1000&lt;/span>*memoryRequest)/memoryCapacity
&lt;span style="color:#999;font-style:italic">// 保证 Burstable 分数高于 Guaranteed
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> &lt;span style="color:#24909d">int&lt;/span>(oomScoreAdjust) &amp;lt; (&lt;span style="color:#3677a9">1000&lt;/span> + guaranteedOOMScoreAdj) {
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> (&lt;span style="color:#3677a9">1000&lt;/span> + guaranteedOOMScoreAdj)
}
&lt;span style="color:#999;font-style:italic">// 保证 Burstable 分数低于 BestEffect
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> &lt;span style="color:#24909d">int&lt;/span>(oomScoreAdjust) == besteffortOOMScoreAdj {
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> &lt;span style="color:#24909d">int&lt;/span>(oomScoreAdjust - &lt;span style="color:#3677a9">1&lt;/span>)
}
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> &lt;span style="color:#24909d">int&lt;/span>(oomScoreAdjust)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="qos-的本质区别">QoS 的本质区别&lt;/h3>
&lt;p>三种 QoS 在调度和实现都存在着区别：&lt;/p>
&lt;ul>
&lt;li>调度时，调度器只会根据 request 值进行调度，这也就解释了有些 Node 节点 Resource Limit 超出 100% 的情况&lt;/li>
&lt;li>当 OOM 时，系统会根据 &lt;code>oom_score&lt;/code> 值来选择优先 kill 掉的进程，分数越高越先被 kill 掉。&lt;code>oom_score&lt;/code> 由系统计算所得，用户是不能设置的。但是如上文所述，而根据 QoS 的类型，kubelet 会计算出 &lt;code>oom_score_adj&lt;/code> 的值，通过 &lt;code>oom_score_adj&lt;/code> 来调整 &lt;code>oom_score&lt;/code> 的分数，从而影响 OOM 被 kill 进程的优先级。&lt;/li>
&lt;li>对于资源的限制，是由 CGroup 来完成的。kubelet 会为三种 QoS 分别创建 QoS level CGroup：
&lt;ul>
&lt;li>&lt;code>Guaranteed&lt;/code> Pod Qos 的 CGroup level 会直接创建在 &lt;code>RootCgroup/kubepods&lt;/code> 下&lt;/li>
&lt;li>&lt;code>Burstable&lt;/code> Pod Qos 的创建在 &lt;code>RootCgroup/kubepods/burstable&lt;/code> 下&lt;/li>
&lt;li>&lt;code>BestEffort&lt;/code> Pod Qos 的创建在 &lt;code>RootCgroup/kubepods/BestEffort&lt;/code>下&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>而在 Pod level CGroup 中还会创建 Container level CGroup，其结构如下图所示：&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://tva3.sinaimg.cn/large/ad5fbf65ly1gjoxdsmfs0j20mr0e8tjg.jpg" alt="Qos-CGroup">&lt;/p>
&lt;h2 id="结语">结语&lt;/h2>
&lt;p>本文我们讨论了 Kubernetes 中 QoS 机制的分类、打分及其本质，除了这些 QoS 的实现 &lt;code>QOSContainerManager&lt;/code> 中还有三种 QoS 以宿主机上 allocatable 资源量为基础为 Pod 分配资源，并通过多个 level cgroup 进行层层限制的逻辑，由于篇幅有限，就不做详细介绍了。&lt;/p></description></item><item><title>Post: 图解 K8S 源码 - Informer 篇（上）</title><link>https://guoxudong.io/post/diagrams-k8s-src-informer/</link><pubDate>Mon, 12 Oct 2020 15:21:14 +0800</pubDate><guid>https://guoxudong.io/post/diagrams-k8s-src-informer/</guid><description>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>众所周知，在 Kubernetes 中各组件是通过 HTTP 协议进行通信的，而组件间的通信也并没有依赖任何中间件，那么如何保证消息的实时性、可靠性、顺序性呢？&lt;strong>Informer 机制&lt;/strong>很好的解决了这个问题。Kubernetes 中各组件与 API Server 的通信都是通过 client-go 的 informer 机制来保证和完成的。&lt;/p>
&lt;h2 id="控制器模式">控制器模式&lt;/h2>
&lt;p>控制器模式最核心的就是控制循环的概念。而 Informer 机制，也就是控制循环中负责观察系统的传感器（Sensor）主要由 Reflector、Informer、Indexer 三个组件构成。其与各种资源的 Controller 相配合，就可以完成完整的控制循环，不断的使系统向终态趋近 &lt;code>status&lt;/code> -&amp;gt; &lt;code>spec&lt;/code>。&lt;/p>
&lt;p>&lt;img src="https://tva2.sinaimg.cn/large/ad5fbf65ly1gjme5nhuykj20mr0fmn6j.jpg" alt="informer 机制">&lt;/p>
&lt;h3 id="informer">Informer&lt;/h3>
&lt;p>所谓 informer，其实就是一个带有本地缓存和索引机制的，可以注册 EventHandler 的 client，目的是为了减轻频繁通信 API Server 的压力而抽取出来的一层 cache，客户端对 API Server 数据的&lt;strong>读取&lt;/strong>和&lt;strong>监测&lt;/strong>操作都通过本地的 informer 来进行。&lt;/p>
&lt;p>每一个 Kubernetes 资源上都实现了 informer 机制，每一个 informer 上都会实现 &lt;code>Informer()&lt;/code> 和 &lt;code>Lister()&lt;/code> 方法：&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#999;font-style:italic">// client-go/informers/core/v1/pod.go
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">type&lt;/span> PodInformer &lt;span style="color:#6ab825;font-weight:bold">interface&lt;/span> {
&lt;span style="color:#447fcf">Informer&lt;/span>() cache.SharedIndexInformer
&lt;span style="color:#447fcf">Lister&lt;/span>() v1.PodLister
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>定义不同资源的 Informer，允许监控不同资源事件。同时为了避免同一资源的 Informer 被实例化多次，而每个 Informer 都会使用一个 Reflector，这样会运行过多相同的 ListAndWatch，从而加重 API Server 的压力，Informer 还提供了共享机制，多个 Informer 可以共享一个 Reflector，从而达到节约资源的目的。&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#999;font-style:italic">// client-go/informers/factory.go
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">type&lt;/span> sharedInformerFactory &lt;span style="color:#6ab825;font-weight:bold">struct&lt;/span> {
client kubernetes.Interface
namespace &lt;span style="color:#6ab825;font-weight:bold">string&lt;/span>
tweakListOptions internalinterfaces.TweakListOptionsFunc
lock sync.Mutex
defaultResync time.Duration
customResync &lt;span style="color:#6ab825;font-weight:bold">map&lt;/span>[reflect.Type]time.Duration
informers &lt;span style="color:#6ab825;font-weight:bold">map&lt;/span>[reflect.Type]cache.SharedIndexInformer
&lt;span style="color:#999;font-style:italic">// startedInformers is used for tracking which informers have been started.
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> &lt;span style="color:#999;font-style:italic">// This allows Start() to be called multiple times safely.
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span> startedInformers &lt;span style="color:#6ab825;font-weight:bold">map&lt;/span>[reflect.Type]&lt;span style="color:#6ab825;font-weight:bold">bool&lt;/span>
}
...
&lt;span style="color:#999;font-style:italic">// InternalInformerFor returns the SharedIndexInformer for obj using an internal client.
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">func&lt;/span> (f *sharedInformerFactory) &lt;span style="color:#447fcf">InformerFor&lt;/span>(obj runtime.Object, newFunc internalinterfaces.NewInformerFunc) cache.SharedIndexInformer {
f.lock.&lt;span style="color:#447fcf">Lock&lt;/span>()
&lt;span style="color:#6ab825;font-weight:bold">defer&lt;/span> f.lock.&lt;span style="color:#447fcf">Unlock&lt;/span>()
informerType := reflect.&lt;span style="color:#447fcf">TypeOf&lt;/span>(obj)
informer, exists := f.informers[informerType]
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> exists {
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> informer
}
resyncPeriod, exists := f.customResync[informerType]
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> !exists {
resyncPeriod = f.defaultResync
}
informer = &lt;span style="color:#447fcf">newFunc&lt;/span>(f.client, resyncPeriod)
f.informers[informerType] = informer
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> informer
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>使用 map 数据结构实现共享 Informer 机制，在 &lt;code>InformerFor()&lt;/code> 函数添加了不同资源的 Informer，在添加过程中如果已经存在同类型的 Informer，则返回当前 Informer，不再继续添加。如下就是 &lt;code>deployment&lt;/code> 的 &lt;code>Informer()&lt;/code> 方法，其中就调用了 &lt;code>InformerFor()&lt;/code> 函数。&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#999;font-style:italic">// client-go/informers/apps/v1beta1/deployment.go
&lt;/span>&lt;span style="color:#999;font-style:italic">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">func&lt;/span> (f *deploymentInformer) &lt;span style="color:#447fcf">Informer&lt;/span>() cache.SharedIndexInformer {
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> f.factory.&lt;span style="color:#447fcf">InformerFor&lt;/span>(&amp;amp;appsv1beta1.Deployment{}, f.defaultInformer)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="reflector">Reflector&lt;/h3>
&lt;p>Reflector 用于监测制定 Kubernetes 资源，当资源发生变化时，触发相应的事件，如：Added（资源添加）事件、Update（资源更新）事件、Delete（资源删除）事件，并将事件及资源名称添加到 DeltaFIFO 中。&lt;/p>
&lt;h4 id="listandwatch">ListAndWatch&lt;/h4>
&lt;p>在实例化 Reflector 时，必须传入 ListerWatcher 接口对象，其拥有 &lt;code>List()&lt;/code> 和 &lt;code>Watch()&lt;/code> 方法。Reflector 通过 &lt;code>Run()&lt;/code> 方法启动监控并处理事件。在程序第一次运行时，会执行 &lt;code>List()&lt;/code> 方法将所有的对象数据存入 DeltaFIFO 中，每次 Controller 重启，都会执行 &lt;code>List()&lt;/code> 方法；同时，Reflector 实例中还有 &lt;code>resyncPeriod&lt;/code> 参数，如果该参数不为 0，则会根据该参数值周期性的执行 &lt;code>List()&lt;/code> 操作，此时这些资源对象会被设置为 Sync 操作类型（不同于 Add、Update 等）。&lt;/p>
&lt;p>而 &lt;code>Watch()&lt;/code> 则会根据 Reflector 实例 &lt;code>period&lt;/code> 参数，周期性的监控资源对象是否有变更。如果发生变更，则通过 &lt;code>r.watchHandler&lt;/code> 处理变更事件。&lt;/p>
&lt;p>&lt;img src="https://tva4.sinaimg.cn/large/ad5fbf65ly1gjmkxmiboej20mr0uwh9f.jpg" alt="Reflector">&lt;/p>
&lt;h4 id="deltafifo">DeltaFIFO&lt;/h4>
&lt;p>DeltaFIFO 顾名思义，Delta 是一个资源对象存储，可以保持操作类型（Add、Update、Delete、Sync等）；而 FIFO 则是一个先进先出的队列。其是一个生产者与消费者的队列，其中 Reflector 是生产者，消费者则调用 &lt;code>Pop()&lt;/code> 方法取出最早进入队列的对象数据。&lt;/p>
&lt;h3 id="indexer">Indexer&lt;/h3>
&lt;p>Indexer 是 client-go 用来存储资源对象并自带索引功能的本地存储，Reflector 从 DeltaFIFO 中将消费出来的资源对象存储至 Indexer。同时 Indexer 中的数据与 Etcd 中的数据保持完全一致。client-go 可以很方便的从本次存储中读取相应的资源对象数据，而无需每次都从远程 Etcd 集群中读取，从而降低了 API Server 和 Etcd 集群的压力。&lt;/p>
&lt;h2 id="结语">结语&lt;/h2>
&lt;p>要了解 Kubernetes，Informer 是绕不过的内容，其在 Kubernetes 中非常重要。本文主要图解了 Informer 机制以及 Reflector，由于篇幅有限，DeltaFIFO，Indexer 等概念只做了简单介绍，这些内容会在后续的文章中进行详解，敬请期待。&lt;/p>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ul>
&lt;li>《Kubernetes 源码剖析 - 郑东旭著》&lt;/li>
&lt;/ul></description></item><item><title>Post: 图解 K8S 源码 - Deployment Controller 篇</title><link>https://guoxudong.io/post/diagrams-k8s-src-deployment-controller/</link><pubDate>Mon, 28 Sep 2020 15:00:38 +0800</pubDate><guid>https://guoxudong.io/post/diagrams-k8s-src-deployment-controller/</guid><description>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>Kubernetes 最为云原生领域的绝对 leader，可以说是当下最著名开源项目之一，拥有着庞大的贡献者群体以及更庞大的用户群体。作为使用 Go 语言开发的明星项目，其源码也是非常有趣的。笔者在研究 Kubernetes 源码时，常常发现很多让人眼前一亮的设计和拍案叫绝的逻辑。但由于 Kubernetes 的代码量十分庞大，函数间的调用也十分复杂，在阅读源码时常常被绕的找不着北，正好手边有一本《图解算法》，于是就萌生了图解 Kubernetes 源码的想法。本文为本系列第一篇文章，尝试使用流程图来分析 Kubernetes Controller Manager 中 的 Deployment Controller 逻辑。&lt;/p>
&lt;h2 id="deployment-controller">Deployment Controller&lt;/h2>
&lt;p>Deployment Controller 是 Kube-Controller-Manager 中最常用的 Controller 之一管理 Deployment 资源。而 Deployment 的本质就是通过管理 ReplicaSet 和 Pod 在 Kubernetes 集群中部署 &lt;strong>无状态&lt;/strong> Workload。&lt;/p>
&lt;h3 id="deploymentreplicaset-和-pod">Deployment、ReplicaSet 和 Pod&lt;/h3>
&lt;p>&lt;img src="https://tvax4.sinaimg.cn/large/ad5fbf65gy1gj6twofn24j20es09s43a.jpg" alt="deployment-controller">&lt;/p>
&lt;p>Deployment 通过控制 ReplicaSet，ReplicaSet 再控制 Pod，最终由 Controller 驱动达到期望状态。在控制器模式下，每次操作对象都会触发一次事件，然后 controller 会进行一次 syncLoop 操作，controller 是通过 informer 监听事件以及进行 ListWatch 操作的。&lt;/p>
&lt;p>Deployment Controller 会监听 DeploymentInformer、ReplicaSetInformer、PodInformer 三种资源。这三种资源变化时，都会触发 syncLoop 也就是下面代码 &lt;code>dc.Run()&lt;/code> 中的 &lt;code>dc.syncDeployment&lt;/code> 操作，来进行状态更新逻辑。&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#6ab825;font-weight:bold">func&lt;/span> &lt;span style="color:#447fcf">startDeploymentController&lt;/span>(ctx ControllerContext) (http.Handler, &lt;span style="color:#6ab825;font-weight:bold">bool&lt;/span>, &lt;span style="color:#6ab825;font-weight:bold">error&lt;/span>) {
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> !ctx.AvailableResources[schema.GroupVersionResource{Group: &lt;span style="color:#ed9d13">&amp;#34;apps&amp;#34;&lt;/span>, Version: &lt;span style="color:#ed9d13">&amp;#34;v1&amp;#34;&lt;/span>, Resource: &lt;span style="color:#ed9d13">&amp;#34;deployments&amp;#34;&lt;/span>}] {
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">nil&lt;/span>, &lt;span style="color:#6ab825;font-weight:bold">false&lt;/span>, &lt;span style="color:#6ab825;font-weight:bold">nil&lt;/span>
}
dc, err := deployment.&lt;span style="color:#447fcf">NewDeploymentController&lt;/span>(
ctx.InformerFactory.&lt;span style="color:#447fcf">Apps&lt;/span>().&lt;span style="color:#447fcf">V1&lt;/span>().&lt;span style="color:#447fcf">Deployments&lt;/span>(),
ctx.InformerFactory.&lt;span style="color:#447fcf">Apps&lt;/span>().&lt;span style="color:#447fcf">V1&lt;/span>().&lt;span style="color:#447fcf">ReplicaSets&lt;/span>(),
ctx.InformerFactory.&lt;span style="color:#447fcf">Core&lt;/span>().&lt;span style="color:#447fcf">V1&lt;/span>().&lt;span style="color:#447fcf">Pods&lt;/span>(),
ctx.ClientBuilder.&lt;span style="color:#447fcf">ClientOrDie&lt;/span>(&lt;span style="color:#ed9d13">&amp;#34;deployment-controller&amp;#34;&lt;/span>),
)
&lt;span style="color:#6ab825;font-weight:bold">if&lt;/span> err != &lt;span style="color:#6ab825;font-weight:bold">nil&lt;/span> {
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">nil&lt;/span>, &lt;span style="color:#6ab825;font-weight:bold">true&lt;/span>, fmt.&lt;span style="color:#447fcf">Errorf&lt;/span>(&lt;span style="color:#ed9d13">&amp;#34;error creating Deployment controller: %v&amp;#34;&lt;/span>, err)
}
&lt;span style="color:#6ab825;font-weight:bold">go&lt;/span> dc.&lt;span style="color:#447fcf">Run&lt;/span>(&lt;span style="color:#24909d">int&lt;/span>(ctx.ComponentConfig.DeploymentController.ConcurrentDeploymentSyncs), ctx.Stop)
&lt;span style="color:#6ab825;font-weight:bold">return&lt;/span> &lt;span style="color:#6ab825;font-weight:bold">nil&lt;/span>, &lt;span style="color:#6ab825;font-weight:bold">true&lt;/span>, &lt;span style="color:#6ab825;font-weight:bold">nil&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="deployment-controller-启动流程">Deployment Controller 启动流程&lt;/h3>
&lt;p>那么先从启动逻辑开始，Kube-Controller-Manager 中所有的 Controller 的启动逻辑都差不多，都是在 &lt;code>Run()&lt;/code> 方法中完成初始化并启动，&lt;code>NewControllerInitializers&lt;/code> 会初始化所有 Controller，而 &lt;code>startXXXXController()&lt;/code> 则会启动对应的 Controller。&lt;/p>
&lt;p>&lt;img src="https://tva3.sinaimg.cn/large/ad5fbf65gy1gj6rw439nrj20mh12o7wh.jpg" alt="deployment-controller-启动流程">&lt;/p>
&lt;h3 id="核心逻辑-synchandler">核心逻辑 syncHandler&lt;/h3>
&lt;p>Deployment Controller 在初始化时指定了 &lt;code>dc.syncHandler = dc.syncDeployment&lt;/code>，所以核心逻辑就是围绕 &lt;code>syncDeployment()&lt;/code> 来展开的。&lt;/p>
&lt;p>&lt;img src="https://tvax4.sinaimg.cn/large/ad5fbf65gy1gj6s4tfuynj20my1zq7wi.jpg" alt="deployment-controller-核心逻辑">&lt;/p>
&lt;p>从源码可以看出，删除、暂停、回滚、扩缩容、更新策略的优先级为 &lt;code>delete &amp;gt; pause &amp;gt; rollback &amp;gt; scale &amp;gt; rollout&lt;/code>。而最终都不是直接更新或修改对应资源，而是通过 &lt;code>dc.client.AppsV1().Deployments().UpdateStatus()&lt;/code> 更新 Deployment Status。&lt;/p>
&lt;h2 id="结语">结语&lt;/h2>
&lt;p>以上就是 Deployment Controller 代码逻辑，通过流程图，希望能描述的更加清晰。因为是第一次尝试图解，可能有遗漏和不足，欢迎留言指正。图解 Kubernetes 源码将作为一个系列继续下去，后续会带来更多的源码图解。&lt;/p></description></item></channel></rss>