<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Guo Xudong's Blog·郭旭东的博客 – rancher</title><link>https://guoxudong.io/tags/rancher/</link><description>Recent content in rancher on Guo Xudong's Blog·郭旭东的博客</description><generator>Hugo -- gohugo.io</generator><language>zh</language><lastBuildDate>Thu, 23 Apr 2020 14:03:53 +0800</lastBuildDate><atom:link href="https://guoxudong.io/tags/rancher/index.xml" rel="self" type="application/rss+xml"/><item><title>Post: 去指挥你的舰队吧！体验使用 Fleet 批量管理 K8S 集群</title><link>https://guoxudong.io/post/rancher-fleet/</link><pubDate>Thu, 23 Apr 2020 14:03:53 +0800</pubDate><guid>https://guoxudong.io/post/rancher-fleet/</guid><description>
&lt;h2 id="前言">前言&lt;/h2>
&lt;div class="alert alert-primary" role="alert">
2020年4月3日，Rancher Labs 宣布推出全新开源项目 Fleet，致力于为用户提供海量 Kubernetes 集群的集中管理体验。
&lt;/div>
&lt;p>最早听说到这个消息时，我还是很疑惑的，Fleet 不是 CoreOS 早已经停止维护的一个项目吗？怎么又和 Rancher Labs 扯上了关系？&lt;/p>
&lt;p>**“为用户提供海量 Kubernetes 集群的集中管理体验”**这句话是否言过其实：&lt;/p>
&lt;ul>
&lt;li>“海量”这个量到底有多大？&lt;/li>
&lt;li>又有多少公司或团队有管理海量的 Kubernetes 集群的需求？&lt;/li>
&lt;li>又是怎么一个&lt;strong>集中管理&lt;/strong>法？&lt;/li>
&lt;/ul>
&lt;p>带着这些疑问，我仔细了解了一下 Fleet 这个开源项目。&lt;/p>
&lt;h2 id="fleet">Fleet&lt;/h2>
&lt;p>首先，这里的 Fleet 是一个新项目，起这个名字应该算是一种致敬，经过了解后我个人觉得这个名字起的还是挺贴切的，比一大波 KubeXXX 有创意多了。&lt;/p>
&lt;blockquote>
&lt;p>“我一直是它的忠实粉丝，将这一项目命名为 Fleet 也包含了我的私心。”Darren Shepherd 解释道：“所以我希望重新使用 Fleet 这一名字，这是对这个非常出色的容器领域早期项目的致敬。同时，对于推动 Kubernetes 集群管理的演进，我们感到十分兴奋及万分期待。”&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>&amp;mdash; 摘自 RancherLabs 官方微信公众号《Rancher开源Fleet：业界首个海量K8S集群管理项目》&lt;/p>
&lt;/blockquote>
&lt;p>顾名思义 Fleet 是“舰队”的意思，而 Kubernetes 在希腊语意为 “舵手”。从名称上看，Fleet 的目标就是管理或是指挥众多 Kubernetes 集群。而在了解这个项目时，我发现了这个项目和 Rancher Labs 另一个受欢迎项目 &lt;a href="https://k3s.io/">k3s&lt;/a> 有个千丝万缕的联系，甚至在我看来 Fleet 可能就是就是为了管理众多 k3s 集群而生的，是 Rancher Labs 布局边缘计算和 IoT 领域的重要组成部分。&lt;/p>
&lt;p>k3s 是一款轻量级的 Kubernetes 集群，主要面向边缘计算和 IOT 领域，相比原生 Kubernetes，k3s 体量更轻、部署简单且快速，同时还具有完整的 Kubernetes 体验。可以说只要是 Linux 系统（配合周边工具甚至可以运行在 Mac 和 Windows 系统），无论是树莓派、各种开发板还是 PC 机，都可以独立运行起 k3s，&lt;strong>这也为运行海量 Kubernetes 集群&lt;/strong>提供了可能。以汽车为例，我们可以为每一辆汽车都部署一个 k3s 集群，所有汽车相关的软件（导航、广播甚至是无人驾驶程序）都部署在 k3s 集群中，每次这些软件发布新版本，只需使用 Fleet 进行批量操作该种车型的所有 k3s 集群即可，无需将车开回 4S 店进行手动更新。&lt;/p>
&lt;div class="alert alert-primary" role="alert">
联系美国空军是 Kubernetes 与 Istio 项目的重要用户，这种实践可能早就开始了。
&lt;/div>
&lt;p>解释了海量 Kubernetes 集群的疑问，下面就从 Fleet 的架构入手，讲讲如何&lt;strong>集中管理&lt;/strong>。&lt;/p>
&lt;p>&lt;img src="https://tva1.sinaimg.cn/large/ad5fbf65ly1ge3o40xe41j20qx0ljdm7.jpg" alt="">&lt;/p>
&lt;p>Fleet 包含&lt;code>Manager&lt;/code>和&lt;code>agent&lt;/code>，&lt;code>Manager&lt;/code>所在集群作为控制平面管理所有&lt;code>agent&lt;/code>集群，同时 Fleet 根据 Kubernetes 部署 Pod 的模型，定义了一个 Bundles 对象，并且提供了一种内置机制，可以使用诸如&lt;code>Helm&lt;/code>和&lt;code>Kustomize&lt;/code>等行业标准工具为每个目标集群定制 Bundles，在我看来这种模式以及&lt;code>bundle.yaml&lt;/code>的写法都和&lt;code>Kustomize&lt;/code>很像(套娃行为？)&amp;hellip;一旦用户在集群之间部署了 Bundles，Fleet 就会主动监视资源是否已就绪，以及是否被更改过。总的来说就是通过部署 Bundles，就可以将部署内容批量分发到所有目标集群，从而达到&lt;strong>集中管理&lt;/strong>的目的。&lt;/p>
&lt;h2 id="尝鲜体验">尝鲜体验&lt;/h2>
&lt;p>说那么多其实意义不大，好不好用，只有试过才知道。这里使用的 Fleet 版本为&lt;code>v0.2.0&lt;/code>，是目前的最新版本。&lt;/p>
&lt;p>&lt;strong>下载 CLI 工具&lt;/strong>&lt;/p>
&lt;p>首先需要下载&lt;code>fleet&lt;/code>的 CLI 工具，这里的体验和 k3s 类似，都是直接&lt;code>curl&lt;/code> GitHub 上的安装脚本并执行：&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ curl -sfL https://raw.githubusercontent.com/rancher/fleet/master/install.sh | sh -
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>部署控制平面&lt;/strong>&lt;/p>
&lt;p>使用 CLI 工具将&lt;code>Fleet Manager&lt;/code>部署到 Kubernetes 集群上：&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#999;font-style:italic"># Kubeconfig should point to Manager cluster&lt;/span>
$ fleet install manager | kubectl apply -f -
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>生成 Cluster group token&lt;/strong>&lt;/p>
&lt;p>到这控制平面就部署好了，接下来部署&lt;code>agent&lt;/code>目标集群。这里生成的其实是一个 yaml 文件，内容包含 fleet 需要的 RBAC 权限和 fleet-agent 的 Deployment：&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#999;font-style:italic"># Kubeconfig should point to Manager cluster&lt;/span>
$ fleet install agent-token &amp;gt; token
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>目标集群注册&lt;/strong>&lt;/p>
&lt;p>将需要纳管的目标集群加入到 fleet 中，&lt;strong>注意&lt;/strong>：这里需要将 kubeconfig 切换到目标集群，也就是需要部署&lt;code>agent&lt;/code>的集，每个需要注册的集群都要部署&lt;code>agent&lt;/code>：&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#999;font-style:italic"># Kubeconfig should point to AGENT cluster&lt;/span>
$ kubectl apply -f token
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>部署 bundles&lt;/strong>&lt;/p>
&lt;p>这里就是向多个集群同时部署 bundles，使用方法也和&lt;code>Kustomize&lt;/code>类似（&lt;code>example&lt;/code> 目录是 fleet 官方仓库中的示例目录）：&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#999;font-style:italic"># Kubeconfig should point to Manager cluster&lt;/span>
$ fleet apply ./examples/helm-kustomize
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>查看状态&lt;/strong>&lt;/p>
&lt;p>现在就可以查看所有集群 bundles 的状态了，这里可以看到 bundles 在多个集群都部署成功了（这里是我起的两个 k3s 集群做的测试）：&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get fleet
NAME CLUSTER-COUNT BUNDLES-READY BUNDLES-DESIRED STATUS
clustergroup.fleet.cattle.io/default &lt;span style="color:#3677a9">2&lt;/span> &lt;span style="color:#3677a9">3&lt;/span> &lt;span style="color:#3677a9">4&lt;/span> Modified: &lt;span style="color:#3677a9">1&lt;/span> (helm-kustomize )
NAME CLUSTERS-READY CLUSTERS-DESIRED STATUS
bundle.fleet.cattle.io/fleet-agent &lt;span style="color:#3677a9">2&lt;/span> &lt;span style="color:#3677a9">2&lt;/span>
bundle.fleet.cattle.io/helm-kustomize &lt;span style="color:#3677a9">1&lt;/span> &lt;span style="color:#3677a9">2&lt;/span> Modified: &lt;span style="color:#3677a9">1&lt;/span> (default-default-group/cluster-5a186072-acbd-4f54-8f22-fb1651ce902f )
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="总结">总结&lt;/h2>
&lt;p>总的来说，Fleet 的架构简洁且十分轻量，部署方式简单，使用&lt;code>YAML&lt;/code>、&lt;code>Helm&lt;/code>、&lt;code>Kustomez&lt;/code>都可以进行资源的描述和配置，甚至可以使用&lt;code>Helm&lt;/code>+&lt;code>Kustomeze&lt;/code>的模式，部署体验不错。&lt;/p>
&lt;p>但遗憾的是，目前 Fleet 还处于项目早期，实践也仅限于尝鲜体验，并不能用于生产环境，项目 README 中还专门提到了&lt;strong>目前 Fleet 仅适用于 10 个集群以下的小规模部署&lt;/strong>。目前文档不足且项目维护人员并不积极，文档勘误的 &lt;a href="https://github.com/rancher/fleet/pull/32">RP&lt;/a> 和相关 ISSUE 也没有得到相关的反馈。项目是做到了业界首个，但是要真正生产可用甚至做到业界第一还有很长的一段路要走。&lt;/p>
&lt;h2 id="参考">参考&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/rancher/fleet">Fleet - Github&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/byErGqVBtm4kdv58OZFt_w">Rancher开源Fleet：业界首个海量K8S集群管理项目 - RancherLabs&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Post: Rancher 2.2.1 解决工作负载监控为空问题</title><link>https://guoxudong.io/post/rancher-prometheus-fix-question/</link><pubDate>Thu, 18 Apr 2019 17:46:08 +0800</pubDate><guid>https://guoxudong.io/post/rancher-prometheus-fix-question/</guid><description>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>Rancher 2.2.X 版本于3月底正式GA，新版本处理其他部分的优化以外，最大亮点莫过于本身集成了 Prometheus ，可以通过 Rancher 自带 UI 或者 Grafana 查看集群的实时监控，对所有监控进行了一次聚合，不用再和之前一样，每个集群都要安装一个 Prometheus 用于监控，而告警部分也可使用 Rancher 自带的通知组件进行告警。通知方式目前支持 Slack 、 邮件、 PagerDuty 、 Webhook 、 企业微信，由于我司办公使用钉钉，所以我们使用了 Webhook 的方式，告警触发后通知我们的消息服务，然后消息服务将其发送到钉钉进行告警。&lt;/p>
&lt;p>&lt;img src="http://wx2.sinaimg.cn/large/ad5fbf65gy1g26xsh6omvj20rk0ilta6.jpg" alt="image">&lt;/p>
&lt;h2 id="问题">问题&lt;/h2>
&lt;p>Rancher 集成 Prometheus 后，监控方面变的十分强大，不用再徘徊于多个集群的 Grafana ，直接在 Rancher 上即可查看，非常方便&lt;/p>
&lt;p>&lt;img src="http://wx2.sinaimg.cn/large/ad5fbf65gy1g26xuv2frnj212b0onn1h.jpg" alt="image">&lt;/p>
&lt;p>但是在使用的时候，我发现了一个问题：就是在查看 工作负载和 Pod 的时候会显示 &lt;em>&lt;strong>没有足够的数据绘制图表&lt;/strong>&lt;/em>&lt;/p>
&lt;p>&lt;img src="http://wx4.sinaimg.cn/large/ad5fbf65gy1g26xzvi2cpj20po057q31.jpg" alt="image">&lt;/p>
&lt;p>进入 Grafana 查看会发现，其实监控参数是存在的，但是没有采集到值，所以并没有展示出来。&lt;/p>
&lt;p>&lt;img src="http://wx4.sinaimg.cn/large/ad5fbf65gy1g26y4j4s3yj21f50m9wqj.jpg" alt="image">&lt;/p>
&lt;h2 id="解决">解决&lt;/h2>
&lt;p>在检查了配置后并没有找到原因，只好去 GitHub 上提一个 issue 来询问一下开发者或者其他用户有无遇到这个问题。&lt;/p>
&lt;p>Rancher 官方的开发者还是十分负责的， GitHub 上用户名为 &lt;a href="https://github.com/loganhz">Logan&lt;/a> 的官方小哥来我指导解决这个问题。&lt;/p>
&lt;p>小哥发现我是导入的集群，要我进入 Prometheus 查看，发现 &lt;code>cattle-prometheus/exporter-kube-state-cluster-monitoring&lt;/code> 果然没有起来&lt;/p>
&lt;p>&lt;img src="http://wx4.sinaimg.cn/large/ad5fbf65gy1g26yb1p4eoj21db0am782.jpg" alt="image">&lt;/p>
&lt;p>解决这个问题，需要在集群监控配置中添加一个高级选项，插入值为：&lt;code>exporter-kubelets.https=false&lt;/code>&lt;/p>
&lt;p>&lt;img src="http://wx4.sinaimg.cn/large/ad5fbf65gy1g26ycq6amfj221q0uggp8.jpg" alt="image">&lt;/p>
&lt;p>点击保存，问题就解决了！&lt;/p>
&lt;p>&lt;img src="http://wx4.sinaimg.cn/large/ad5fbf65gy1g26yheqwp7j213e0g3di5.jpg" alt="image">&lt;/p>
&lt;h2 id="后记">后记&lt;/h2>
&lt;p>使用 Rancher 有半年，从2.0版本一直用到2.2版本，而18年分别在云栖大会和 KubeCon 上听了 Rancher 创始人梁胜博士的演讲。而从这一个小问题上就可以看到 Rancher 官方对每一个用户都是十分重视的。&lt;/p></description></item><item><title>Post: 单节点版rancher升级指南</title><link>https://guoxudong.io/post/rancher-update-2.2.1/</link><pubDate>Sun, 31 Mar 2019 11:15:35 +0800</pubDate><guid>https://guoxudong.io/post/rancher-update-2.2.1/</guid><description>
&lt;blockquote>
&lt;p>Rancher 不仅可以在任何云提供商的任何地方部署 Kubernetes 集群，而且还将它们集中在集中式的身份验证和访问控制之下。由于它与资源的运行位置无关，因此您可以轻松地在不同的环境部署你的 kubernetes 集群并操作他们。 Rancher 不是将部署几个独立的 Kubernetes 集群，而是将它们统一为一个单独的托管Kubernetes Cloud。&lt;/p>
&lt;/blockquote>
&lt;h2 id="前言">前言&lt;/h2>
&lt;p>目前我们使用的是 rancher 2.1.1版本，在去年 rancher 发布 &lt;code>v2.1.*&lt;/code> 版本的时候做过一次升级，当时遇到了很多问题，虽然都一一解决，但是并没有有效的记录下来，这里在升级 &lt;code>v2.2.*&lt;/code> 版本的时候做一个记录以便在今后升级的时候的提供参考作用。&lt;/p>
&lt;h2 id="升级前的准备">升级前的准备&lt;/h2>
&lt;ul>
&lt;li>首先查看当前 rancher 版本，记下这个版本号后面需要使用。查看方式就是登陆 rancher 在左下角就可以看到当前版本号，我们这里使用的&lt;code>v2.1.1&lt;/code>版本。&lt;/li>
&lt;li>打开官方文档，这里推荐对照官方文档进行升级，一般官方文档都会及时更新并提供最佳升级方法，而一般的博客会因为其写作时间、使用版本、部署环境的不同有所偏差。官方文档： &lt;a href="https://www.cnrancher.com/docs/rancher/v2.x/cn/upgrades/single-node-upgrade/">https://www.cnrancher.com/docs/rancher/v2.x/cn/upgrades/single-node-upgrade/&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="升级">升级&lt;/h2>
&lt;ol>
&lt;li>
&lt;p>首先获取正在运行的 rancher 容器 ID,由以下命令可知 &lt;code>RANCHER_CONTAINER_ID&lt;/code> 为 &lt;code>83167cb60134&lt;/code>&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ docker ps
CONTAINER ID IMAGE COMMAND CREATED STATUS
PORTS NAMES
83167cb60134 rancher/rancher:latest &lt;span style="color:#ed9d13">&amp;#34;entrypoint.sh&amp;#34;&lt;/span> &lt;span style="color:#3677a9">4&lt;/span> months ago Up &lt;span style="color:#3677a9">4&lt;/span> months 0.0.0.0:80-&amp;gt;80/tcp, 0.0.0.0:443-&amp;gt;443/tcp priceless_newton
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>停止该容器&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ docker stop {RANCHER_CONTAINER_ID}
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>创建正在运行的 Rancher Server 容器的数据卷容器，将在升级中使用，这里命名为 &lt;code>rancher-data&lt;/code> 容器。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>替换{RANCHER_CONTAINER_ID}为上一步中的容器ID。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>替换{RANCHER_CONTAINER_TAG}为你当前正在运行的Rancher版本，如上面的先决条件中所述。&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ docker create --volumes-from {RANCHER_CONTAINER_ID} --name rancher-data rancher/rancher:{RANCHER_CONTAINER_TAG}
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>备份 &lt;code>rancher-data&lt;/code> 数据卷容器&lt;/p>
&lt;p>如果升级失败，可以通过此备份还原Rancher Server，容器命名:rancher-data-snapshot-&amp;lt;CURRENT_VERSION&amp;gt;.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>替换{RANCHER_CONTAINER_ID}为上一步中的容器ID。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>替换{CURRENT_VERSION}为当前安装的Rancher版本的标记。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>替换{RANCHER_CONTAINER_TAG}为当前正在运行的Rancher版本，如先决条件中所述 。&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ docker create --volumes-from {RANCHER_CONTAINER_ID} --name rancher-data-snapshot-{CURRENT_VERSION} rancher/rancher:{RANCHER_CONTAINER_TAG}
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>拉取Rancher的最新镜像,这里确保有外网，可能拉取到新的镜像，如果没有外网，这里就需要将镜像上传到私有镜像仓库，将拉取地址设置为私有镜像仓库即可&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ docker pull rancher/rancher:latest
&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>通过 &lt;code>rancher-data&lt;/code> 数据卷容器启动新的 Rancher Server 容器。&lt;/p>
&lt;p>&lt;strong>这里要注意到，我们这是使用的是独立容器+外部七层负载均衡，是通过阿里云SLB进行SSL证书认证，需要在启动的时候增加&lt;code>--no-cacerts&lt;/code>&lt;/strong>&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ docker run -d --volumes-from rancher-data --restart=unless-stopped -p 80:80 -p 443:443 rancher/rancher:latest --no-cacerts
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>升级过程会需要一定时间，不要在升级过程中终止升级，强制终止可能会导致数据库迁移错误。&lt;/p>
&lt;/blockquote>
&lt;blockquote>
&lt;p>升级 Rancher Server后， server 容器中的数据会保存到 &lt;code>rancher-data&lt;/code> 容器中，以便将来升级。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>
&lt;p>删除旧版本 Rancher Server 容器&lt;/p>
&lt;p>如果你只是停止以前的Rancher Server容器(并且不删除它),则旧版本容器可能随着主机重启后自动运行，导致容器端口冲突。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>升级成功&lt;/p>
&lt;p>访问 rancher 可以看到右下角版本已经完成更新。&lt;/p>
&lt;p>&lt;img src="http://wx4.sinaimg.cn/large/ad5fbf65gy1g1lzcmucn6j20ck03qt8p.jpg" alt="image">&lt;/p>
&lt;/li>
&lt;/ol></description></item><item><title>Post: 为ingress配置SSL证书，实现HTTPS访问</title><link>https://guoxudong.io/post/https-ingress/</link><pubDate>Sat, 29 Dec 2018 21:28:13 +0800</pubDate><guid>https://guoxudong.io/post/https-ingress/</guid><description>
&lt;blockquote>
&lt;p>devops平台率先在公司内使用kubernetes集群提供后端服务，但是由于之前一直处于探索阶段，所以使用的事http的方式提供后端服务，但是在开发统一入口后，出现了访问HTTPS页面的跨域问题，由此引出了后端服务配置SSL证书的问题&lt;/p>
&lt;/blockquote>
&lt;h1 id="使用rancher配置ssl证书">使用rancher配置SSL证书&lt;/h1>
&lt;h2 id="下载ssl证书文件">下载SSL证书文件&lt;/h2>
&lt;p>首先需要获得SSL证书文件，可以直接在阿里云SSL证书管理控制台下载&lt;/p>
&lt;p>选中需要下载证书，选择下载nginx证书
&lt;img src="https://guoxudong.io/images/source/zhengshu.png" alt="image">&lt;/p>
&lt;h2 id="将证书上传项目">将证书上传项目&lt;/h2>
&lt;p>打开rancher，选择要使用证书的项目，点击资源中的证书&lt;/p>
&lt;h2 id="将证书上传项目-1">将证书上传项目&lt;/h2>
&lt;p>打开rancher，选择要使用证书的项目，点击资源中的证书
&lt;img src="https://guoxudong.io/images/source/https-1.png" alt="image">
添加证书，点击从文件上传
&lt;img src="https://guoxudong.io/images/source/https-2.png" alt="image">
上传证书文件中的秘钥和证书，点击保存即可&lt;/p>
&lt;h1 id="使用yaml上传证书">使用yaml上传证书&lt;/h1>
&lt;p>这个证书的原理其实是在相应的命名空间创建了一个包含证书信息的secrets&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#6ab825;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#666"> &lt;/span>v1&lt;span style="color:#666">
&lt;/span>&lt;span style="color:#666">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">data&lt;/span>:&lt;span style="color:#666">
&lt;/span>&lt;span style="color:#666"> &lt;/span>&lt;span style="color:#6ab825;font-weight:bold">tls.crt&lt;/span>:&lt;span style="color:#666"> &lt;/span>{私钥}&lt;span style="color:#666">
&lt;/span>&lt;span style="color:#666"> &lt;/span>&lt;span style="color:#6ab825;font-weight:bold">tls.key&lt;/span>:&lt;span style="color:#666"> &lt;/span>{证书}&lt;span style="color:#666">
&lt;/span>&lt;span style="color:#666">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">kind&lt;/span>:&lt;span style="color:#666"> &lt;/span>Secret&lt;span style="color:#666">
&lt;/span>&lt;span style="color:#666">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#666">
&lt;/span>&lt;span style="color:#666"> &lt;/span>&lt;span style="color:#6ab825;font-weight:bold">name&lt;/span>:&lt;span style="color:#666"> &lt;/span>keking-cn&lt;span style="color:#666">
&lt;/span>&lt;span style="color:#666"> &lt;/span>&lt;span style="color:#6ab825;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#666"> &lt;/span>devops-plat&lt;span style="color:#666">
&lt;/span>&lt;span style="color:#666">&lt;/span>&lt;span style="color:#6ab825;font-weight:bold">type&lt;/span>:&lt;span style="color:#666"> &lt;/span>kubernetes.io/tls&lt;span style="color:#666">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>在kubernetes上运行该yaml即可&lt;/p>
&lt;h1 id="rancher中证书绑定">rancher中证书绑定&lt;/h1>
&lt;p>选中需要绑定证书的ingress，点击编辑，选中证书，保存即可（由于ingress-controller中没有绑定默认证书，所以这里不能选中默认）
&lt;img src="https://guoxudong.io/images/source/https-3.png" alt="image">
保存完毕，证书即可生效&lt;/p></description></item><item><title>Post: 阿里云部署rancher2.1采坑记</title><link>https://guoxudong.io/post/install-rancher/</link><pubDate>Thu, 29 Nov 2018 18:28:13 +0800</pubDate><guid>https://guoxudong.io/post/install-rancher/</guid><description>
&lt;blockquote>
&lt;p>近期由于公司需要将部署在ucloud上的rancher迁移到阿里云上，所以将部署到阿里云的图中遇到的问题和踩到的坑在这里进行记录。&lt;/p>
&lt;/blockquote>
&lt;h1 id="无法删除namespace">无法删除namespace&lt;/h1>
&lt;p>在安装新环境的rancher之前，需要将kubernetes集群中cattle-system ns下面的cluster-agent和node-agent干掉，这里我选择直接删除cattle-system这个命名空间&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl delete ns cattle-system
&lt;/code>&lt;/pre>&lt;/div>&lt;p>然而问题来了，在删除命名空间之后，这个命名空间并没有立刻被删除，而是一直处于Terminating状态，这里我专门写了一篇文章解决这个问题，这里就不再赘述&lt;/p>
&lt;h1 id="阿里云证书配置">阿里云证书配置&lt;/h1>
&lt;p>由于之前使用的ucloud的机器进行测试，使用默认自签名证书并没有使用SSL证书，所以在配置证书这里遇到的许多的问题&lt;/p>
&lt;p>首先根据官方文档使用权威CA机构颁发的证书，这里使用的是本公司自己的证书&lt;/p>
&lt;p>获取证书方法：
&lt;img src="https://guoxudong.io/images/source/jinrussl.png" alt="image">&lt;/p>
&lt;p>点击下载证书，选择nginx证书下载
&lt;img src="https://guoxudong.io/images/source/zhengshu.png" alt="image">&lt;/p>
&lt;p>之后将下载的证书上传到rancher所在服务器，并配置好数据卷挂载&lt;/p>
&lt;p>将下面代码的挂载地址指向证书文件，运行代码&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ docker run -d --restart=unless-stopped &lt;span style="color:#ed9d13">\
&lt;/span>&lt;span style="color:#ed9d13">&lt;/span>-p 80:80 -p 443:443 &lt;span style="color:#ed9d13">\
&lt;/span>&lt;span style="color:#ed9d13">&lt;/span>-v /root/var/log/auditlog:/var/log/auditlog &lt;span style="color:#ed9d13">\
&lt;/span>&lt;span style="color:#ed9d13">&lt;/span>-e &lt;span style="color:#40ffff">AUDIT_LEVEL&lt;/span>=&lt;span style="color:#3677a9">3&lt;/span> &lt;span style="color:#ed9d13">\
&lt;/span>&lt;span style="color:#ed9d13">&lt;/span>-v /etc/your_certificate_directory/fullchain.pem:/etc/rancher/ssl/cert.pem &lt;span style="color:#ed9d13">\
&lt;/span>&lt;span style="color:#ed9d13">&lt;/span>-v /etc/your_certificate_directory/privkey.pem:/etc/rancher/ssl/key.pem &lt;span style="color:#ed9d13">\
&lt;/span>&lt;span style="color:#ed9d13">&lt;/span>rancher/rancher:latest --no-cacerts
&lt;/code>&lt;/pre>&lt;/div>&lt;p>之后会自动冲dockerhub上拉取最新的rancher进行进行安装，之后使用命令&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">docker ps
&lt;/code>&lt;/pre>&lt;/div>&lt;p>查看容器是否在运行，如果运行正常，则后端的配置就完成了&lt;/p>
&lt;p>划重点：这是是在后端配置了证书，所以在阿里云的配置上要使用四层TCP监听&lt;/p>
&lt;p>这个地方可是坑了我许久，我一直在前端配置https七层监听，导致一直无法正常访问，一度已经到了怀疑人生的地步=。=&lt;/p>
&lt;p>之后就是简单的阿里云SLB配置四层TCP监听，这里也就不再赘述了&lt;/p>
&lt;h1 id="k8s集群导入rancher">k8s集群导入rancher&lt;/h1>
&lt;p>前后端都准备就绪，现在就可以访问rancher了，访问rancher根据页面提示进行基本配置，登录后选择添加集群&lt;/p>
&lt;p>选择导入现有集群
&lt;img src="https://guoxudong.io/images/source/add.png" alt="image">&lt;/p>
&lt;p>为集群创建一个rancher中的名称，然后根据提示将命令拷贝到k8s集群所在宿主机执行即可，注意：这里由于配置了证书，所以选择有证书，不绕过证书的那个命令执行，之后就可看到集群数据导入中
&lt;img src="https://guoxudong.io/images/source/wating.png" alt="image">&lt;/p>
&lt;p>等待几秒即可开心的使用rancher了！&lt;/p>
&lt;h1 id="关于rancher部署后访问集群api超时问题">关于rancher部署后访问集群api超时问题&lt;/h1>
&lt;p>经过排查，原因是阿里云在容器服务对外连接处设置了TLS双向认证，导致rancher的外网ip经常性的被拦截，导致超时&lt;/p>
&lt;p>解决办法：&lt;/p>
&lt;p>对k8s集群中rancher的cattle-cluster-agent传递内网参数，将其配置为内网连接，就可以正常访问了&lt;/p>
&lt;div class="highlight">&lt;pre style="color:#d0d0d0;background-color:#202020;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">kubectl -n cattle-system patch deployments cattle-cluster-agent --patch &lt;span style="color:#ed9d13">&amp;#39;{
&lt;/span>&lt;span style="color:#ed9d13"> &amp;#34;spec&amp;#34;: {
&lt;/span>&lt;span style="color:#ed9d13"> &amp;#34;template&amp;#34;: {
&lt;/span>&lt;span style="color:#ed9d13"> &amp;#34;spec&amp;#34;: {
&lt;/span>&lt;span style="color:#ed9d13"> &amp;#34;hostAliases&amp;#34;: [{
&lt;/span>&lt;span style="color:#ed9d13"> &amp;#34;hostnames&amp;#34;:[&amp;#34;rancher.keking.cn&amp;#34;], #rancher的域名
&lt;/span>&lt;span style="color:#ed9d13"> &amp;#34;ip&amp;#34;: &amp;#34;10.0.0.219&amp;#34; #rancher部署地址
&lt;/span>&lt;span style="color:#ed9d13"> }]
&lt;/span>&lt;span style="color:#ed9d13"> }
&lt;/span>&lt;span style="color:#ed9d13"> }
&lt;/span>&lt;span style="color:#ed9d13"> }
&lt;/span>&lt;span style="color:#ed9d13">}&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div></description></item></channel></rss>